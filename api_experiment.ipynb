{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import dotenv\n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chemical formula for water is H‚ÇÇO. This indicates that each molecule of water is composed of two hydrogen atoms (H) and one oxygen atom (O).\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "prompt = \"What is the chemical formula for water?\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": prompt\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "    ],\n",
    "\n",
    "    # Optional parameters\n",
    "    temperature=0.3,\n",
    "    max_tokens=1024,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0.1,\n",
    "    presence_penalty=0.1,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GeminiAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chemical formula of glucose is **C‚ÇÜH‚ÇÅ‚ÇÇO‚ÇÜ**. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "import dotenv\n",
    "\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# Set up the model\n",
    "generation_config = {\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_p\": 0.8,\n",
    "    \"top_k\": 64,\n",
    "    \"max_output_tokens\": 8192,\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-pro\",\n",
    "    generation_config=generation_config,\n",
    ")\n",
    "\n",
    "response = model.generate_content(\"What is the chemical formula of glucose?\")\n",
    "\n",
    "try:\n",
    "    print(response.text)\n",
    "except Exception as e:\n",
    "    print(\"Exception:\\n\", e, \"\\n\")\n",
    "    print(\"Response:\\n\", response.candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the food items in the fridge and their quantities, to the best of my ability to determine from the image:\n",
      "\n",
      "Top Shelf:\n",
      "\n",
      "* Avocados: 4-5 (partially obscured)\n",
      "* Green Onions: Large bunch\n",
      "* Mushrooms: 6-7\n",
      "* Fresh Herbs: 1 bunch\n",
      "* Salmon Fillet: 1 \n",
      "\n",
      "Bottom Shelf:\n",
      "\n",
      "* Salad Greens: 1 container\n",
      "* Edible Flowers: 6-8\n",
      "* Apples: 3\n",
      "* Oranges: 2\n",
      "* Lemon: 1\n",
      "* Kiwi Fruit: 2 \n",
      "\n",
      "Please note:  It's difficult to be exact with quantities for some items as they are partially hidden. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Chat with files\n",
    "\n",
    "generation_config = {\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_p\": 0.8,\n",
    "    \"top_k\": 64,\n",
    "    \"max_output_tokens\": 8192,\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-pro\",\n",
    "    generation_config=generation_config,\n",
    ")\n",
    "\n",
    "prompt_parts = [\n",
    "    genai.upload_file(\"fridge_food.jpg\"),\n",
    "    \"List the food items in the fridge and their quantities.\"\n",
    "]\n",
    "\n",
    "response = model.generate_content(prompt_parts)\n",
    "\n",
    "try:\n",
    "    print(response.text)\n",
    "except Exception as e:\n",
    "    print(\"Exception:\\n\", e, \"\\n\")\n",
    "    print(\"Response:\\n\", response.candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Les grands mod√®les de langage sont g√©niaux !\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Chat history\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "chat_history = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [\"Hi!\"]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"model\",\n",
    "        \"parts\": [\"Hi there! How can I help you today?\"],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [\"Translate 'Large Language Models are awesome!' to French.\"],\n",
    "    }\n",
    "]\n",
    "\n",
    "response = model.generate_content(chat_history)\n",
    "\n",
    "try:\n",
    "    print(response.text)\n",
    "except Exception as e:\n",
    "    print(\"Exception:\\n\", e, \"\\n\")\n",
    "    print(\"Response:\\n\", response.candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's awesome! Pizza is a classic for a reason. What's your favorite kind of pizza?  üçï \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Chat history auto\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\", generation_config={\"temperature\": 0.3})\n",
    "chat = model.start_chat(history=[])\n",
    "     \n",
    "prompt_parts = [\"My favourite food is pizza.\"]\n",
    "response = chat.send_message(prompt_parts)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI, I don't have access to your personal information, including your favorite food.  You told me your favorite food is pizza, though!  üòä  \n",
      "\n",
      "Is there anything else you'd like to tell me about your favorite food?  Maybe what kind of pizza you like best, or what you like about it? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_parts = [\"What is my favourite food?\"]\n",
    "response = chat.send_message(prompt_parts)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[parts {\n",
       "   text: \"My favourite food is pizza.\"\n",
       " }\n",
       " role: \"user\",\n",
       " parts {\n",
       "   text: \"That\\'s awesome! Pizza is a classic for a reason. What\\'s your favorite kind of pizza?  üçï \\n\"\n",
       " }\n",
       " role: \"model\",\n",
       " parts {\n",
       "   text: \"What is my favourite food?\"\n",
       " }\n",
       " role: \"user\",\n",
       " parts {\n",
       "   text: \"As an AI, I don\\'t have access to your personal information, including your favorite food.  You told me your favorite food is pizza, though!  üòä  \\n\\nIs there anything else you\\'d like to tell me about your favorite food?  Maybe what kind of pizza you like best, or what you like about it? \\n\"\n",
       " }\n",
       " role: \"model\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anthropic Claude API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shortest protein in the human body is generally considered to be thymosin beta-4, which consists of only 43 amino acids. However, it's important to note a few points:\n",
      "\n",
      "1. Definition of protein: Some very small peptides might not always be classified as proteins by all researchers, so there can be some debate about what constitutes the \"shortest protein.\"\n",
      "\n",
      "2. Variability: The human body produces thousands of proteins, and our understanding of the proteome is still evolving. New discoveries could potentially identify even shorter functional proteins.\n",
      "\n",
      "3. Function: Despite its small size, thymosin beta-4 plays important roles in the body, including promoting cell migration, blood vessel formation, and wound healing.\n",
      "\n",
      "4. Other small proteins: There are other very small proteins in the human body, such as:\n",
      "   - Insulin-like growth factor II (67 amino acids)\n",
      "   - Insulin (51 amino acids)\n",
      "\n",
      "5. Synthetic proteins: Researchers have created even shorter synthetic proteins, but these are not naturally occurring in the human body.\n",
      "\n",
      "In summary, while thymosin beta-4 is generally recognized as the shortest protein in the human body, ongoing research in proteomics may lead to new discoveries in this area.\n"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "import dotenv\n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "client = anthropic.Anthropic(\n",
    "    # defaults to os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "    api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    ")\n",
    "\n",
    "message = client.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0.3,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"What is the shortest protein in the human body?\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above code we had to wait for the full responce from the LLM.<br>\n",
    "Below we actually use the stream method to start reading the responce as we get it without having to wait for the full responce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I\n",
      "---\n",
      " assist you today?\n",
      "---\n",
      " Feel\n",
      "---\n",
      " free to ask any\n",
      "---\n",
      " questions or\n",
      "---\n",
      " let me know if you\n",
      "---\n",
      " need help with anything\n",
      "---\n",
      ".\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "with client.messages.stream(\n",
    "    max_tokens=1024,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    ") as stream:\n",
    "    for text in stream.text_stream:\n",
    "        print(text)\n",
    "        print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to encode image to base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def file_to_base64(file):\n",
    "    return base64.b64encode(open(file, \"rb\").read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the image, this appears to be a Neapolitan-style pizza. Here are the likely ingredients and basic instructions to make a similar pizza:\n",
      "\n",
      "Ingredients:\n",
      "1. Pizza dough\n",
      "2. Tomato sauce\n",
      "3. Fresh mozzarella cheese\n",
      "4. Italian sausage or meatballs (crumbled)\n",
      "5. Fresh basil leaves\n",
      "6. Olive oil (not visible but typically used)\n",
      "\n",
      "Instructions:\n",
      "1. Preheat your oven to the highest temperature, ideally with a pizza stone or steel inside.\n",
      "\n",
      "2. Stretch out your pizza dough into a round shape on a floured surface.\n",
      "\n",
      "3. Spread a thin layer of tomato sauce over the dough, leaving a small border around the edges.\n",
      "\n",
      "4. Tear fresh mozzarella into pieces and distribute evenly over the sauce.\n",
      "\n",
      "5. Add small pieces of crumbled sausage or meatballs across the pizza.\n",
      "\n",
      "6. Drizzle with a little olive oil.\n",
      "\n",
      "7. Carefully transfer the pizza to the preheated stone or steel in the oven.\n",
      "\n",
      "8. Bake for about 2-3 minutes for a Neapolitan-style pizza, or until the crust is puffed and charred in spots, and the cheese is melted.\n",
      "\n",
      "9. Remove from the oven and immediately add fresh basil leaves.\n",
      "\n",
      "10. Slice and serve hot.\n",
      "\n",
      "The key to achieving this look is using high-quality, fresh ingredients and cooking at a very high temperature for a short time to get that characteristic charred yet chewy crust.\n"
     ]
    }
   ],
   "source": [
    "image_media_type = \"image/jpeg\"\n",
    "image_data = file_to_base64(\"pizza.jpeg\")\n",
    "\n",
    "message = client.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0.3,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            # content is a list which now passes and image and text\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"source\": {\n",
    "                        \"type\": \"base64\",\n",
    "                        \"media_type\": image_media_type,\n",
    "                        \"data\": image_data,\n",
    "                    },\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Tell me the ingredients and the instructions to make the pizza from the image.\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
